{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23532,"status":"ok","timestamp":1669844033274,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"},"user_tz":300},"id":"HX0rp0thaM_a","outputId":"0d3b48e1-fc3f-48c7-8d3b-2216eb449998"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = 'EMChat/EmChat'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15332,"status":"ok","timestamp":1669844048602,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"},"user_tz":300},"id":"3lHVsyczavuc","outputId":"4fc306c6-267a-4a54-83f6-27cdc1f402f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-ignite\n","  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 30.6 MB/s \n","\u001b[?25hRequirement already satisfied: torch\u003c2,\u003e=1.3 in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite) (1.12.1+cu113)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch\u003c2,\u003e=1.3-\u003epytorch-ignite) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging-\u003epytorch-ignite) (3.0.9)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.4.10\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting boto3\n","  Downloading boto3-1.26.20-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 30.5 MB/s \n","\u001b[?25hCollecting s3transfer\u003c0.7.0,\u003e=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 10.8 MB/s \n","\u001b[?25hCollecting botocore\u003c1.30.0,\u003e=1.29.20\n","  Downloading botocore-1.29.20-py3-none-any.whl (10.2 MB)\n","\u001b[K     |████████████████████████████████| 10.2 MB 57.8 MB/s \n","\u001b[?25hCollecting jmespath\u003c2.0.0,\u003e=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: python-dateutil\u003c3.0.0,\u003e=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore\u003c1.30.0,\u003e=1.29.20-\u003eboto3) (2.8.2)\n","Collecting urllib3\u003c1.27,\u003e=1.25.4\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 67.4 MB/s \n","\u001b[?25hRequirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil\u003c3.0.0,\u003e=2.1-\u003ebotocore\u003c1.30.0,\u003e=1.29.20-\u003eboto3) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1, but you have urllib3 1.26.13 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.26.20 botocore-1.29.20 jmespath-1.0.1 s3transfer-0.6.0 urllib3-1.26.13\n"]}],"source":["!pip3 install pytorch-ignite\n","!pip3 install boto3"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20661,"status":"ok","timestamp":1669844069261,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"},"user_tz":300},"id":"0ocD8R-KaXbx","outputId":"af3256a5-055c-4333-9d25-a63719f941a5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n","  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"]}],"source":["# Copyright (c) 2019-present, HuggingFace Inc.\n","# All rights reserved. This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.\n","import os\n","import math\n","import logging\n","from pprint import pformat\n","from argparse import ArgumentParser\n","from collections import defaultdict\n","from itertools import chain\n","import torch.nn.functional as F\n","from config import InteractConfig\n","\n","import torch\n","from torch.nn.parallel import DistributedDataParallel\n","from torch.utils.data import DataLoader, TensorDataset\n","from ignite.engine import Engine, Events\n","from ignite.handlers import ModelCheckpoint\n","from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n","from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n","from config import Config\n","from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n","from ignite.contrib.handlers.tensorboard_logger import *\n","\n","from pytorch_pretrained_bert import (OpenAIAdam, OpenAIGPTDoubleHeadLMEmotionRecognitionModel, OpenAIGPTTokenizer,\n","                                     GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME,\n","                                     BertModel, BertTokenizer)\n","\n","from utils import get_dataset, get_dataset_for_daily_dialog"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1669844069262,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"},"user_tz":300},"id":"8ulEWUHHaXfq","outputId":"273c0b19-5b0a-43fb-daa7-64aedf10f2a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:This is a warning message\n"]}],"source":["SPECIAL_TOKENS = [\"\u003cbos\u003e\", \"\u003ceos\u003e\", \"\u003cspeaker1\u003e\", \"\u003cspeaker2\u003e\",\n","                  \"\u003cno_emotion\u003e\", \"\u003chappiness\u003e\", \"\u003csurprise\u003e\", \"\u003csadness\u003e\", \"\u003cdisgust\u003e\", \"\u003canger\u003e\", \"\u003cfear\u003e\",\n","                  \"\u003cdirective\u003e\", \"\u003cinform\u003e\", \"\u003ccommissive\u003e\", \"\u003cquestion\u003e\",\n","                  \"\u003cpad\u003e\"]\n","MODEL_INPUTS  = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\", \"token_emotion_ids\"]\n","PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\", \"token_emotion_ids\"]\n","\n","\n","logger = logging.getLogger()\n","fhandler = logging.FileHandler(filename='iteract_emotion.log', mode='a')\n","logger.addHandler(fhandler)\n","logging.warning('This is a warning message')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1669844069262,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"},"user_tz":300},"id":"DZS9VS00aXmc"},"outputs":[],"source":["def average_distributed_scalar(scalar, config):\n","    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n","    if config.local_rank == -1:\n","        return scalar\n","    scalar_t = torch.tensor(scalar, dtype=torch.float, device=config.device) / torch.distributed.get_world_size()\n","    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n","    return scalar_t.item()\n","\n","\n","def pad_dataset(dataset, padding=0):\n","    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padd only batches but this is simpler. \"\"\"\n","    max_l = max(len(x) for x in dataset[\"input_ids\"])\n","    for name in PADDED_INPUTS:\n","        dataset[name] = [x + [padding if name != \"lm_labels\" else -1] * (max_l - len(x)) for x in dataset[name]]\n","    return dataset\n","\n","\n","def get_emotion_label(tokenizer, candidate_emotion):\n","    _, _, _, _, no_emotion_id, happiness_id, surprise_id, sadness_id, disgust_id, anger_id, fear_id, _, _, _, _, _ = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","    if candidate_emotion == happiness_id:\n","        return 0\n","    elif candidate_emotion == surprise_id:\n","        return 1\n","    elif candidate_emotion == sadness_id:\n","        return 2\n","    elif candidate_emotion == disgust_id:\n","        return 3\n","    elif candidate_emotion == anger_id:\n","        return 4\n","    elif candidate_emotion == fear_id:\n","        return 5\n","    elif candidate_emotion == no_emotion_id:\n","        return 6\n","\n","\n","def build_input_from_segments(history, emotions, reply, tokenizer, with_eos=True):\n","    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply \"\"\"\n","    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:4])\n","    #tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1])\n","\n","    instance = {}\n","    # sequence = [[bos] + history[0] + list(chain(*history[1:]))]  + [reply + ([eos] if with_eos else [])] #seq = [personas, history, reply] concatenate all persona sentences\n","    sequence = [[bos] + history[0]] + history[1:] + [reply + ([eos] if with_eos else [])]\n","    sequence = [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence)]\n","\n","    instance[\"input_ids\"] = list(chain(*sequence))\n","    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s] # the last for is for repeating the speaker1 and speaker2 for all tokens\n","    #instance[\"token_emotion_ids\"] = [emotions[i] for i, s in enumerate(sequence[:-1]) for _ in s] + [true_emotion] * len(sequence[-1])\n","    instance[\"token_emotion_ids\"] = [emotions[i] for i, s in enumerate(sequence[:-1]) for _ in range(len(s)+1)]\n","\n","    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n","    # instance[\"mc_labels\"] = get_emotion_label(tokenizer, true_emotion)\n","    # instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:] #all -1 except for reply, reply is just the ids\n","    return instance, sequence\n","\n","\n","\n","def top_filtering(logits, top_k=0, top_p=0.0, threshold=-float('Inf'), filter_value=-float('Inf')):\n","    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n","        Args:\n","            logits: logits distribution shape (..., vocabulary size)\n","            top_k: \u003c=0: no filtering, \u003e0: keep only top k tokens with highest probability.\n","            top_p: \u003c=0.0: no filtering, \u003e0.0: keep only a subset S of candidates, where S is the smallest subset\n","                whose total probability mass is greater than or equal to the threshold top_p.\n","                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n","                the threshold top_p.\n","            threshold: a minimal threshold to keep logits\n","    \"\"\"\n","    top_k = min(top_k, logits.size(-1))\n","    if top_k \u003e 0:\n","        # Remove all tokens with a probability less than the last token in the top-k tokens\n","        indices_to_remove = logits \u003c torch.topk(logits, top_k)[0][..., -1, None]\n","        logits[indices_to_remove] = filter_value\n","\n","    if top_p \u003e 0.0:\n","        # Compute cumulative probabilities of sorted tokens\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","        # Remove tokens with cumulative probability above the threshold\n","        sorted_indices_to_remove = cumulative_probabilities \u003e top_p\n","        # Shift the indices to the right to keep also the first token above the threshold\n","        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","        sorted_indices_to_remove[..., 0] = 0\n","\n","        # Back to unsorted indices and set them to -infinity\n","        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","        logits[indices_to_remove] = filter_value\n","\n","    indices_to_remove = logits \u003c threshold\n","    logits[indices_to_remove] = filter_value\n","\n","    return logits\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1669844069263,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"},"user_tz":300},"id":"b0XlD9FcapR9"},"outputs":[],"source":["\n","EMOTIONS = [\"HAPPINESS\", \"SURPRISE\", \"SADNESS\", \"DISGUST\", \"ANGRY\", \"FEAR\"]\n","EMOTIONS_TOKEN_ID = {\"NO_EMOTION\":40482, \"HAPPINESS\":40483, \"SURPRISE\":40484, \"SADNESS\":40485, \"DISGUST\":40486, \"ANGRY\":40487, \"FEAR\":40488}\n","\n","def sample_sequence(history, tokenizer, model, args, SPECIAL_TOKENS, reply=None, emotion_history=None):\n","    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","\n","    if reply is None:\n","        reply = []\n","\n","    candidate_emotion = []\n","    candidate_act = []\n","\n","\n","    for i in range(args.max_length):\n","        topic = 40492\n","        emotions = emotion_history\n","        actions = [40499]*len(history)\n","        instance, sequence = build_input_from_segments(history, emotions, reply, tokenizer,\n","                                                       with_eos=False)\n","\n","        input_ids = torch.tensor(instance[\"input_ids\"], device=args.device).unsqueeze(0)\n","        mc_token_ids = torch.tensor(instance[\"mc_token_ids\"], device=args.device).unsqueeze(0)\n","        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args.device).unsqueeze(0)\n","        token_emotion_ids = torch.tensor(instance[\"token_emotion_ids\"], device=args.device).unsqueeze(0)\n","\n","        lm_logits, mc_logits  = model(input_ids, mc_token_ids, token_type_ids=token_type_ids, token_emotion_ids=token_emotion_ids)\n","\n","        emotion_indices = torch.argmax(mc_logits, dim=1)\n","        \n","        # logits = logits.squeeze(0)\n","        # if \"gpt2\" == args.model:\n","        #     logits = logits[0]\n","        # logits = logits[0, -1, :] / args.temperature\n","        # logits = top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n","        # probs = F.softmax(logits, dim=-1)\n","\n","        # prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n","        # if i \u003c args.min_length and prev.item() in special_tokens_ids:\n","        #     while prev.item() in special_tokens_ids:\n","        #         prev = torch.multinomial(probs, num_samples=1)\n","\n","        # if prev.item() in special_tokens_ids:\n","        #     break\n","        # reply.append(prev.item())\n","\n","        candidate_emotion.append(EMOTIONS[emotion_indices[0]])\n","\n","    return candidate_emotion\n","\n","\n","def run():\n","    # config_file = \"configs/interact_emotion_config.json\"\n","    config_file = \"/content/drive/MyDrive/EMChat/EmChat/configs/interact_emotion_config.json\"\n","    config = InteractConfig.from_json_file(config_file)\n","\n","    logging.basicConfig(level=logging.INFO)\n","    # logger = logging.getLogger(__file__)\n","    logger.info(pformat(config))\n","\n","    if config.model_checkpoint == \"\":\n","        config.model_checkpoint = download_pretrained_model()\n","\n","    torch.random.manual_seed(config.seed)\n","    torch.cuda.manual_seed(config.seed)\n","\n","    logger.info(\"Get pretrained model and tokenizer\")\n","    if config.model == \"bert\":\n","        tokenizer_class = BertTokenizer\n","        model_class = BertLMHeadModel\n","    elif config.model == \"gpt2\":\n","        tokenizer_class = GPT2Tokenizer\n","        model_class = GPT2LMHeadModel\n","    else:\n","        tokenizer_class = OpenAIGPTTokenizer\n","        model_class = OpenAIGPTDoubleHeadLMEmotionRecognitionModel\n","\n","\n","    tokenizer = tokenizer_class.from_pretrained(config.model_checkpoint)\n","    model = model_class.from_pretrained(config.model_checkpoint)\n","\n","    model.to(config.device)\n","    model.eval()\n","    out_ids = None\n","    candidate_emotion = None\n","    emotion_history = []\n","    history = [] \n","    while True:\n","        raw_text = input(\"\u003e\u003e\u003e \")\n","        while not raw_text:\n","            print('Prompt should not be empty!')\n","            raw_text = input(\"\u003e\u003e\u003e \")\n","        # if out_ids != None:\n","        #     history.append(out_ids)\n","        #     emotion_history.append(EMOTIONS_TOKEN_ID[candidate_emotion[len(candidate_emotion)-1]])\n","        history.append(tokenizer.encode(raw_text))\n","        emotion_history.append(EMOTIONS_TOKEN_ID[\"HAPPINESS\"])\n","        with torch.no_grad():\n","             candidate_emotion = sample_sequence(history, tokenizer, model, config, SPECIAL_TOKENS, None, emotion_history)\n","        # history = history[-(2 * config.max_history + 1):]\n","        # emotion_history = emotion_history[-(2 * config.max_history + 1):]\n","        history.clear()\n","        emotion_history.clear()\n","        # out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n","        print(f\"{candidate_emotion[len(candidate_emotion)-1]}\")\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Kl3ty8D9aY-v"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:pytorch_pretrained_bert.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy \u0026 ftfy.\n"]},{"name":"stdout","output_type":"stream","text":["HAPPINESS\n","HAPPINESS\n","SADNESS\n","HAPPINESS\n","HAPPINESS\n","SADNESS\n","FEAR\n","HAPPINESS\n","HAPPINESS\n","SURPRISE\n","HAPPINESS\n","SADNESS\n","SADNESS\n","HAPPINESS\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-7-4a5b8788c965\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-6-16500d2b6683\u003e\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 88\u001b[0;31m         \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\u003e\u003e\u003e \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prompt should not be empty!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--\u003e 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["if __name__ == \"__main__\":\n","    run()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPG5AuY8hmszeTaLpmcfSWs","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}