{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2ZH14C1JhAr3654k0B9h2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvPBorlcIfVm","executionInfo":{"status":"ok","timestamp":1669788520974,"user_tz":300,"elapsed":20922,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"6648890d-1a97-4c3a-e68c-155fcd298e72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = 'EMChat/EmChat'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n"]},{"cell_type":"code","source":["config_file = \"configs/train_multihead_config.json\"\n"],"metadata":{"id":"01sv8hcs81Kf","executionInfo":{"status":"ok","timestamp":1669788520974,"user_tz":300,"elapsed":2,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip3 install pytorch-ignite\n","!pip3 install boto3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Npa24XB7QzZj","executionInfo":{"status":"ok","timestamp":1669788535977,"user_tz":300,"elapsed":15005,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"ed631904-d012-43c0-b009-85957082b2b4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-ignite\n","  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 30.4 MB/s \n","\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.12.1+cu113)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-ignite) (3.0.9)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.4.10\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting boto3\n","  Downloading boto3-1.26.19-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 31.0 MB/s \n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.19\n","  Downloading botocore-1.29.19-py3-none-any.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 60.8 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 75.5 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.19->boto3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.19->boto3) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.13 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.26.19 botocore-1.29.19 jmespath-1.0.1 s3transfer-0.6.0 urllib3-1.26.13\n"]}]},{"cell_type":"code","source":["import logging\n","logger = logging.getLogger()\n","fhandler = logging.FileHandler(filename='test_log.log', mode='a')\n","logger.addHandler(fhandler)\n","logging.warning('This is a warning message')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zgfsGt8O6HM","executionInfo":{"status":"ok","timestamp":1669788535977,"user_tz":300,"elapsed":4,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"b62a4d7b-bb58-49f0-9a9b-c7bbe62102dc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:This is a warning message\n"]}]},{"cell_type":"code","source":["# Copyright (c) 2019-present, HuggingFace Inc.\n","# All rights reserved. This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.\n","import os\n","import math\n","import logging\n","from pprint import pformat\n","from argparse import ArgumentParser\n","from collections import defaultdict\n","from itertools import chain\n","\n","import torch\n","from torch.nn.parallel import DistributedDataParallel\n","from torch.utils.data import DataLoader, TensorDataset\n","from ignite.engine import Engine, Events\n","from ignite.handlers import ModelCheckpoint\n","from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n","from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n","from config import Config\n","from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n","from ignite.contrib.handlers.tensorboard_logger import *\n","\n","from pytorch_pretrained_bert import (OpenAIAdam, OpenAIGPTMultiHeadModel, OpenAIGPTTokenizer,\n","                                     GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME,\n","                                     BertModel, BertTokenizer)\n","\n","from utils import get_dataset, get_dataset_for_daily_dialog"],"metadata":{"id":"UPgmdG5LIti8","executionInfo":{"status":"ok","timestamp":1669788551452,"user_tz":300,"elapsed":15477,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1720b1a2-e86a-4ae3-ca4f-eb4afa0cfeac"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n","  RequestsDependencyWarning)\n"]}]},{"cell_type":"code","source":["SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\",\n","\n","                  \"<no_emotion>\", \"<happiness>\", \"<surprise>\", \"<sadness>\", \"<disgust>\", \"<anger>\", \"<fear>\",\n","\n","                  \"<work>\", \"<finance>\", \"<relationship>\", \"<attitude_and_emotion>\", \"<culture_and_education>\",\n","                  \"<school_life>\", \"<tourism>\", \"<ordinary_life>\", \"<politics>\", \"<health>\",\n","\n","                  \"<directive>\", \"<inform>\", \"<commissive>\", \"<question>\",\n","                  \"<pad>\"]\n","MODEL_INPUTS = [\"input_ids\", \"ec_token_ids\", \"sc_token_ids\", \"lm_labels\", \"ec_labels\", \"sc_labels\",\n","                \"token_type_ids\", \"token_emotion_ids\", \"token_action_ids\"]\n","PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\", \"token_emotion_ids\", \"token_action_ids\"]\n","\n","# logger = logging.getLogger(__file__)"],"metadata":{"id":"WJfMhfjBIwE3","executionInfo":{"status":"ok","timestamp":1669788551453,"user_tz":300,"elapsed":3,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def average_distributed_scalar(scalar, config):\n","    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n","    if config.local_rank == -1:\n","        return scalar\n","    scalar_t = torch.tensor(scalar, dtype=torch.float, device=config.device) / torch.distributed.get_world_size()\n","    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n","    return scalar_t.item()\n","\n","\n","def pad_dataset(dataset, padding=0):\n","    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padd only batches but this is simpler. \"\"\"\n","    max_l = max(len(x) for x in dataset[\"input_ids\"])\n","    for name in PADDED_INPUTS:\n","        dataset[name] = [x + [padding if name != \"lm_labels\" else -1] * (max_l - len(x)) for x in dataset[name]]\n","    return dataset\n","\n","\n","def get_emotion_label(tokenizer, candidate_emotion):\n","    no_emotion_id, happiness_id, surprise_id, sadness_id, disgust_id, anger_id, fear_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[4:11])\n","\n","    if candidate_emotion == no_emotion_id:\n","        return 0\n","    elif candidate_emotion == happiness_id:\n","        return 1\n","    elif candidate_emotion == surprise_id:\n","        return 2\n","    elif candidate_emotion == sadness_id:\n","        return 3\n","    elif candidate_emotion == disgust_id:\n","        return 4\n","    elif candidate_emotion == anger_id:\n","        return 5\n","    elif candidate_emotion == fear_id:\n","        return 6\n","\n","def build_input_from_segments(topic, history, emotions, actions, reply, candidate_emotion,  canidate_act, tokenizer, lm_labels=False, with_eos=True):\n","    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply \"\"\"\n","    bos, eos, speaker1, speaker2, no_emotion = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:5])\n","\n","    inform = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-4])\n","    emotions = [no_emotion] + emotions\n","    actions = [inform] + actions\n","\n","    instance = {}\n","    sequence = [[bos] + [topic]] + history + [reply + ([eos] if with_eos else [])]\n","    sequence = [[speaker2 if (len(sequence) - i) % 2 else speaker1] + s for i, s in enumerate(sequence)]\n","\n","    instance[\"input_ids\"] = list(chain(*sequence))\n","    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in\n","                                  s]  # the last for is for repeating the speaker1 and speaker2 for all tokens\n","    instance[\"token_emotion_ids\"] = [emotions[i] for i, s in enumerate(sequence[:-1]) for _ in s] + [\n","        candidate_emotion] * len(sequence[-1])\n","    instance[\"token_action_ids\"] = [actions[i] for i, s in enumerate(sequence[:-1]) for _ in s] + [canidate_act] * len(\n","        sequence[-1])\n","\n","    instance[\"ec_token_ids\"] = len(instance[\"input_ids\"]) - 1\n","    instance[\"sc_token_ids\"] = len(instance[\"input_ids\"]) - 2\n","    instance[\"ec_labels\"] = -1\n","    instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n","    if lm_labels:\n","        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][\n","                                                                                     1:]  # all -1 except for reply, reply is just the ids\n","        instance[\"ec_labels\"] = get_emotion_label(tokenizer, candidate_emotion)\n","    return instance, sequence\n","\n","\n","def get_data_loaders(config, tokenizer):\n","    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n","    personachat = get_dataset_for_daily_dialog(tokenizer, config.dataset_path, config.dataset_cache, SPECIAL_TOKENS)\n","\n","    logger.info(\"Build inputs and labels\")\n","    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n","    gpu_max_length = 310\n","    for dataset_name, dataset in personachat.items():\n","        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n","        if config.num_candidates > 0 and dataset_name == 'train':\n","            num_candidates = min(config.num_candidates, num_candidates)\n","        for dialog in dataset:\n","            # topic = dialog[\"topic\"]\n","            topic = 40492\n","            for utterance in dialog[\"utterances\"]:\n","                history = utterance[\"history\"][-(2 * config.max_history + 1):]\n","                emotions = utterance[\"emotion\"][-(2 * config.max_history + 1):]\n","                actions = utterance[\"act\"][-(2 * config.max_history + 1):]\n","                for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n","                    lm_labels = bool(\n","                        j == num_candidates - 1)  # the true label is always the last one in list of candidates\n","                    candidate_emotion = utterance['candidates_emotions'][j]\n","                    candidate_act = utterance['candidates_acts'][j]\n","                    instance, _ = build_input_from_segments(topic, history, emotions, actions, candidate,\n","                                                            candidate_emotion, candidate_act, tokenizer, lm_labels)\n","\n","                    if len(instance[\"input_ids\"]) > gpu_max_length:\n","                        truncated_history = [hist[:10] for hist in history]\n","                        truncated_candidate = candidate[:10]\n","                        instance, _ = build_input_from_segments(topic, truncated_history, emotions, actions,\n","                                                                truncated_candidate,\n","                                                                candidate_emotion, candidate_act, tokenizer, lm_labels)\n","\n","                    for input_name, input_array in instance.items():\n","                        datasets[dataset_name][input_name].append(input_array)\n","\n","                datasets[dataset_name][\"sc_labels\"].append(num_candidates - 1)\n","                datasets[dataset_name][\"n_candidates\"] = num_candidates\n","\n","    logger.info(\"Pad inputs and convert to Tensor\")\n","    tensor_datasets = {\"train\": [], \"valid\": []}\n","    for dataset_name, dataset in datasets.items():\n","        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n","        for input_name in MODEL_INPUTS:\n","            print(input_name)\n","            tensor = torch.tensor(dataset[input_name])\n","            if input_name != \"sc_labels\":\n","                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n","            tensor_datasets[dataset_name].append(tensor)\n","\n","    logger.info(\"Build train and validation dataloaders\")\n","    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n","    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if config.distributed else None\n","    valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if config.distributed else None\n","    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=config.train_batch_size, shuffle=False)\n","    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=config.valid_batch_size, shuffle=False)\n","\n","    logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n","    logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n","    return train_loader, valid_loader, train_sampler, valid_sampler\n","\n","\n"],"metadata":{"id":"KDOmv0ZjIols","executionInfo":{"status":"ok","timestamp":1669788551453,"user_tz":300,"elapsed":3,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def modify_config():\n","    config_file = \"configs/train_emotion_recognition_config.json\"\n","    config = Config.from_json_file(config_file)\n","    config.dataset_path = 0\n","    config.dataset_cache = 0\n","    config.log_dir = 0\n","    config.model_checkpoint = 0\n","\n","    return config\n","\n","def train():\n","    # config_file = \"configs/train_multihead_config.json\"\n","    config_file = \"/content/drive/MyDrive/EMChat/EmChat/configs/interact_multihead_config.json\"\n","    config = Config.from_json_file(config_file)\n","\n","    ec_coef = 1\n","    sc_coef = 1\n","\n","    # logging is set to INFO (resp. WARN) for main (resp. auxiliary) process. logger.info => log main process only, logger.warning => log all processes\n","    logging.basicConfig(level=logging.INFO if config.local_rank in [-1, 0] else logging.WARN)\n","    logger.warning(\"Running process %d\",\n","                   config.local_rank)  # This is a logger.warning: it will be printed by all distributed processes\n","    logger.info(\"Arguments: %s\", pformat(config))\n","\n","    # Initialize distributed training if needed\n","    config.distributed = (config.local_rank != -1)\n","    if config.distributed:\n","        torch.cuda.set_device(config.local_rank)\n","        config.device = torch.device(\"cuda\", config.local_rank)\n","        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n","\n","    logger.info(\"Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning\")\n","    tokenizer_class = OpenAIGPTTokenizer\n","    tokenizer = tokenizer_class.from_pretrained(config.model_checkpoint)\n","    model_class = OpenAIGPTMultiHeadModel\n","    model = model_class.from_pretrained(config.model_checkpoint)\n","    tokenizer.set_special_tokens(SPECIAL_TOKENS)\n","    model.set_num_special_tokens(len(SPECIAL_TOKENS))\n","    model.to(config.device)\n","    optimizer = OpenAIAdam(model.parameters(), lr=config.lr)\n","\n","    # Prepare model for FP16 and distributed training if needed (order is important, distributed should be the last)\n","    if config.fp16:\n","        from apex import amp  # Apex is only required if we use fp16 training\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=config.fp16)\n","    if config.distributed:\n","        model = DistributedDataParallel(model, device_ids=[config.local_rank], output_device=config.local_rank)\n","\n","    logger.info(\"Prepare datasets\")\n","    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(config, tokenizer)\n","\n","    # Training function and trainer\n","    def update(engine, batch):\n","        model.train()\n","        # input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids, token_emotion_ids, token_action_ids = tuple(input_tensor.to(config.device) for input_tensor in batch)\n","        input_ids, ec_token_ids, sc_token_ids, lm_labels, ec_labels, sc_labels, token_type_ids, token_emotion_ids, token_action_ids = tuple(\n","            input_tensor.to(config.device) for input_tensor in batch)\n","\n","        lm_loss, emotion_loss, sentence_loss = model(input_ids, ec_token_ids, sc_token_ids,\n","                                                     lm_labels, ec_labels, sc_labels, token_type_ids,\n","                                                     token_emotion_ids, token_action_ids)\n","        loss = (lm_loss * config.lm_coef + emotion_loss * ec_coef + sentence_loss * sc_coef) / config.gradient_accumulation_steps\n","        if config.fp16:\n","            with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), config.max_norm)\n","        else:\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_norm)\n","        if engine.state.iteration % config.gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        return loss.item()\n","\n","    trainer = Engine(update)\n","\n","    # Evaluation function and evaluator (evaluator output is the input of the metrics)\n","    def inference(engine, batch):\n","        model.eval()\n","        with torch.no_grad():\n","            batch = tuple(input_tensor.to(config.device) for input_tensor in batch)\n","            input_ids, ec_token_ids, sc_token_ids, lm_labels, ec_labels, \\\n","            sc_labels, token_type_ids, token_emotion_ids, token_action_ids = batch\n","            # logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n","            model_outputs = model(input_ids, ec_token_ids, sc_token_ids, token_type_ids=token_type_ids,\n","                                  token_emotion_ids=token_emotion_ids,\n","                                  token_action_ids=token_action_ids)\n","            lm_logits, mc_logits = model_outputs[0], model_outputs[2]  # So we can also use GPT2 outputs\n","            lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n","            lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n","            return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, sc_labels)\n","\n","    evaluator = Engine(inference)\n","\n","    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n","    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n","    if config.n_epochs < 1:\n","        trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))\n","    if config.eval_before_start:\n","        trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n","\n","    # Make sure distributed data samplers split the dataset nicely between the distributed processes\n","    if config.distributed:\n","        trainer.add_event_handler(Events.EPOCH_STARTED, lambda engine: train_sampler.set_epoch(engine.state.epoch))\n","        evaluator.add_event_handler(Events.EPOCH_STARTED, lambda engine: valid_sampler.set_epoch(engine.state.epoch))\n","\n","    # Linearly decrease the learning rate from lr to zero\n","    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, config.lr), (config.n_epochs * len(train_loader), 0.0)])\n","    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n","\n","    # Prepare metrics - note how we compute distributed metrics\n","    RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n","    metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-1), output_transform=lambda x: (x[0][0], x[1][0])),\n","               \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n","    metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], config),\n","                    \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], config)})\n","    metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n","    for name, metric in metrics.items():\n","        metric.attach(evaluator, name)\n","\n","    # On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n","    if config.local_rank in [-1, 0]:\n","        pbar = ProgressBar(persist=True)\n","        pbar.attach(trainer, metric_names=[\"loss\"])\n","        evaluator.add_event_handler(Events.COMPLETED,\n","                                    lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n","\n","        tb_logger = TensorboardLogger(log_dir=config.log_dir)\n","        tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]),\n","                         event_name=Events.ITERATION_COMPLETED)\n","        tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n","        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()),\n","                                                              global_step_transform=global_step_from_engine(trainer)),\n","                         event_name=Events.EPOCH_COMPLETED)\n","\n","        checkpoint_handler = ModelCheckpoint(tb_logger.writer.log_dir, 'checkpoint', save_interval=1, n_saved=3)\n","        trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {\n","            'mymodel': getattr(model, 'module', model)})  # \"getattr\" take care of distributed encapsulation\n","\n","        torch.save(config, tb_logger.writer.log_dir + '/model_training_args.bin')\n","        getattr(model, 'module', model).config.to_json_file(os.path.join(tb_logger.writer.log_dir, CONFIG_NAME))\n","        tokenizer.save_vocabulary(tb_logger.writer.log_dir)\n","\n","    # Run the training\n","    trainer.run(train_loader, max_epochs=config.n_epochs)\n","\n","    # On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n","    if config.local_rank in [-1, 0] and config.n_epochs > 0:\n","        os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(tb_logger.writer.log_dir,\n","                                                                     WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n","        tb_logger.close()\n","\n","\n"],"metadata":{"id":"6Lx14TNiI859","executionInfo":{"status":"ok","timestamp":1669788551453,"user_tz":300,"elapsed":3,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","if __name__ == \"__main__\":\n","    train()"],"metadata":{"id":"dc8dIDE9I1LE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"229a39e0-efe5-41be-de1c-9f6edee8df5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Running process -1\n","WARNING:pytorch_pretrained_bert.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n","WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"]},{"output_type":"stream","name":"stdout","text":["input_ids\n","ec_token_ids\n","sc_token_ids\n","lm_labels\n","ec_labels\n","sc_labels\n","token_type_ids\n","token_emotion_ids\n","token_action_ids\n"]}]}]}