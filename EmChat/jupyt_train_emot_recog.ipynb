{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":396},"id":"dgj0hLi2Ckcu","executionInfo":{"status":"error","timestamp":1670386536711,"user_tz":300,"elapsed":648,"user":{"displayName":"Venkata Tej Kiran Reddy Polamreddy","userId":"00833435530500939066"}},"outputId":"474c2f00-747b-4c90-fc26-ec31f43387e0"},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-f5cd0f1de0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This mounts your Google Drive to the Colab VM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: Enter the foldername in your Drive where you have saved the unzipped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    113\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/mp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is unsupported in this environment.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmountpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: google.colab.drive is unsupported in this environment."]}],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = 'EMChat/EmChat'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"]},{"cell_type":"code","source":["!pip3 install pytorch-ignite\n","!pip3 install boto3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFSQLx2lDB6z","executionInfo":{"status":"ok","timestamp":1670386373320,"user_tz":300,"elapsed":6026,"user":{"displayName":"Venkata Tej Kiran Reddy Polamreddy","userId":"00833435530500939066"}},"outputId":"b928a17a-6a80-4ee8-b330-0f3b1df50c7e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.10)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (21.3)\n","Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-ignite) (3.0.7)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.26.24)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.24 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.29.24)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.6.0)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.24->boto3) (1.26.13)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.24->boto3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.24->boto3) (1.15.0)\n"]}]},{"cell_type":"code","source":["# Copyright (c) 2019-present, HuggingFace Inc.\n","# All rights reserved. This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.\n","import os\n","import math\n","import logging\n","from pprint import pformat\n","from argparse import ArgumentParser\n","from collections import defaultdict\n","from itertools import chain\n","\n","import torch\n","from torch.nn.parallel import DistributedDataParallel\n","from torch.utils.data import DataLoader, TensorDataset\n","from ignite.engine import Engine, Events\n","from ignite.handlers import ModelCheckpoint\n","from ignite.metrics import Accuracy, Recall, Loss, MetricsLambda, RunningAverage, Precision, ConfusionMatrix\n","from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n","from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n","from ignite.contrib.handlers.tensorboard_logger import *\n","\n","from config import Config\n","from pytorch_pretrained_bert import (OpenAIAdam, OpenAIGPTDoubleHeadLMEmotionRecognitionModel, OpenAIGPTTokenizer,\n","                                     GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n","\n","from utils import get_dataset, get_dataset_for_daily_dialog"],"metadata":{"id":"qRDdu7iIDwRu","executionInfo":{"status":"error","timestamp":1670386380724,"user_tz":300,"elapsed":377,"user":{"displayName":"Venkata Tej Kiran Reddy Polamreddy","userId":"00833435530500939066"}},"colab":{"base_uri":"https://localhost:8080/","height":390},"outputId":"a4be9f44-6c11-42f1-9fc1-d6c265a99dd0"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-d0467d76bec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mignite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard_logger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m from pytorch_pretrained_bert import (OpenAIAdam, OpenAIGPTDoubleHeadLMEmotionRecognitionModel, OpenAIGPTTokenizer,\n\u001b[1;32m     23\u001b[0m                                      GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\",\n","                  \"<no_emotion>\", \"<happiness>\", \"<surprise>\", \"<sadness>\", \"<disgust>\", \"<anger>\", \"<fear>\",\n","                  \"<directive>\", \"<inform>\", \"<commissive>\", \"<question>\",\n","                  \"<pad>\"]\n","MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\", \"token_emotion_ids\"]\n","PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\", \"token_emotion_ids\"]\n","\n","logger = logging.getLogger()\n","fhandler = logging.FileHandler(filename='test_log_2.log', mode='a')\n","logger.addHandler(fhandler)\n","logging.warning('This is a warning message')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TBLx6Eh4DxGn","executionInfo":{"status":"ok","timestamp":1669787390396,"user_tz":300,"elapsed":4,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"cc1a952d-47b5-496a-cbe1-27688e723cec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:This is a warning message\n"]}]},{"cell_type":"code","source":["def average_distributed_scalar(scalar, config):\n","    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n","    if config.local_rank == -1:\n","        return scalar\n","    scalar_t = torch.tensor(scalar, dtype=torch.float, device=config.device) / torch.distributed.get_world_size()\n","    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n","    return scalar_t.item()\n","\n","\n","def pad_dataset(dataset, padding=0):\n","    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padd only batches but this is simpler. \"\"\"\n","    max_l = max(len(x) for x in dataset[\"input_ids\"])\n","    for name in PADDED_INPUTS:\n","        dataset[name] = [x + [padding if name != \"lm_labels\" else -1] * (max_l - len(x)) for x in dataset[name]]\n","    return dataset\n","\n","\n","def get_emotion_label(tokenizer, candidate_emotion):\n","    _, _, _, _, no_emotion_id, happiness_id, surprise_id, sadness_id, disgust_id, anger_id, fear_id, _, _, _, _, _ = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","    if candidate_emotion == happiness_id:\n","        return 0\n","    elif candidate_emotion == surprise_id:\n","        return 1\n","    elif candidate_emotion == sadness_id:\n","        return 2\n","    elif candidate_emotion == disgust_id:\n","        return 3\n","    elif candidate_emotion == anger_id:\n","        return 4\n","    elif candidate_emotion == fear_id:\n","        return 5\n","    elif candidate_emotion == no_emotion_id:\n","        return 6\n","\n","\n","def build_input_from_segments(history, emotions, reply, true_emotion, tokenizer, with_eos=True):\n","    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply \"\"\"\n","    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:4])\n","    #tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1])\n","\n","    instance = {}\n","    # sequence = [[bos] + history[0] + list(chain(*history[1:]))]  + [reply + ([eos] if with_eos else [])] #seq = [personas, history, reply] concatenate all persona sentences\n","    sequence = [[bos] + history[0]] + history[1:] + [reply + ([eos] if with_eos else [])]\n","    sequence = [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence)]\n","\n","    instance[\"input_ids\"] = list(chain(*sequence))\n","    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s] # the last for is for repeating the speaker1 and speaker2 for all tokens\n","    #instance[\"token_emotion_ids\"] = [emotions[i] for i, s in enumerate(sequence[:-1]) for _ in s] + [true_emotion] * len(sequence[-1])\n","    instance[\"token_emotion_ids\"] = [emotions[i] for i, s in enumerate(sequence[:-1]) for _ in s]\n","\n","    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n","    instance[\"mc_labels\"] = get_emotion_label(tokenizer, true_emotion)\n","    instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:] #all -1 except for reply, reply is just the ids\n","    return instance, sequence\n","\n","\n","def get_data_loaders(config, tokenizer):\n","    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n","    personachat = get_dataset_for_daily_dialog(tokenizer, config.dataset_path, config.dataset_cache, SPECIAL_TOKENS)\n","\n","    # personachat[\"train\"] = personachat[\"train\"][:100]\n","    # personachat[\"valid\"] = personachat[\"valid\"][:10]\n","\n","    logger.info(\"Build inputs and labels\")\n","    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n","    gpu_max_length = 310\n","    for dataset_name, dataset in personachat.items():\n","        num_candidates = 2#len(dataset[0][\"utterances\"][0][\"candidates\"])\n","        if config.num_candidates > 0 and dataset_name == 'train':\n","            num_candidates = min(config.num_candidates, num_candidates)\n","        for dialog in dataset:\n","            for utterance in dialog[\"utterances\"]:\n","                history = utterance[\"history\"][-(2 * config.max_history + 1):]\n","                emotions = utterance[\"emotion\"][-(2 * config.max_history + 1):]\n","                reply = utterance[\"candidates\"][-1]\n","                true_emotion = utterance['candidates_emotions'][-1]\n","                if true_emotion == tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)[4]:\n","                   continue\n","                instance, _ = build_input_from_segments(history,\n","                                                        emotions,\n","                                                        reply,\n","                                                        true_emotion,\n","                                                        tokenizer)\n","\n","                if len(instance[\"input_ids\"]) > gpu_max_length:\n","                    truncated_history = [hist[:10] for hist in history]\n","                    truncated_candidate = reply[:10]\n","                    true_emotion = utterance['candidates_emotions'][-1]\n","                    instance, _ = build_input_from_segments(truncated_history,\n","                                                            emotions,\n","                                                            truncated_candidate,\n","                                                            true_emotion,\n","                                                            tokenizer)\n","\n","\n","                for input_name, input_array in instance.items():\n","                    datasets[dataset_name][input_name].append(input_array)\n","\n","                datasets[dataset_name][\"n_candidates\"] = num_candidates\n","\n","    logger.info(\"Pad inputs and convert to Tensor\")\n","    tensor_datasets = {\"train\": [], \"valid\": []}\n","    for dataset_name, dataset in datasets.items():\n","        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n","        for input_name in MODEL_INPUTS:\n","            tensor = torch.tensor(dataset[input_name])\n","            #if input_name != \"mc_labels\":\n","            #    tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n","            tensor_datasets[dataset_name].append(tensor)\n","\n","    logger.info(\"Build train and validation dataloaders\")\n","    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n","    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if config.distributed else None\n","    valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if config.distributed else None\n","    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=config.train_batch_size, shuffle=False)\n","    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=config.valid_batch_size, shuffle=False)\n","\n","    logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n","    logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n","    return train_loader, valid_loader, train_sampler, valid_sampler\n","\n","def modify_config():\n","    config_file = \"configs/train_emotion_recognition_config.json\"\n","    config = Config.from_json_file(config_file)\n","    config.dataset_path = 0\n","    config.dataset_cache = 0\n","    config.log_dir = 0\n","    config.model_checkpoint = 0\n","\n","    return config\n","\n","def train():\n","    config_file = \"/content/drive/MyDrive/EMChat/EmChat/configs/train_emotion_recognition_config.json\"\n","    # config_file = \"configs/train_emotion_recognition_config.json\"\n","    config = Config.from_json_file(config_file)\n","\n","    # logging is set to INFO (resp. WARN) for main (resp. auxiliary) process. logger.info => log main process only, logger.warning => log all processes\n","    logging.basicConfig(level=logging.INFO if config.local_rank in [-1, 0] else logging.WARN)\n","    logger.warning(\"Running process %d\", config.local_rank)  # This is a logger.warning: it will be printed by all distributed processes\n","    logger.info(\"Arguments: %s\", pformat(config))\n","\n","    # Initialize distributed training if needed\n","    config.distributed = (config.local_rank != -1)\n","    if config.distributed:\n","        torch.cuda.set_device(config.local_rank)\n","        config.device = torch.device(\"cuda\", config.local_rank)\n","        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n","\n","    logger.info(\"Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning\")\n","    tokenizer_class = GPT2Tokenizer if \"gpt2\" in config.model_checkpoint else OpenAIGPTTokenizer\n","    tokenizer = tokenizer_class.from_pretrained(config.model_checkpoint)\n","    model_class = OpenAIGPTDoubleHeadLMEmotionRecognitionModel\n","    model = model_class.from_pretrained(config.model_checkpoint)\n","    tokenizer.set_special_tokens(SPECIAL_TOKENS)\n","    model.set_num_special_tokens(len(SPECIAL_TOKENS))\n","    model.to(config.device)\n","    optimizer = OpenAIAdam(model.parameters(), lr=config.lr)\n","\n","    # Prepare model for FP16 and distributed training if needed (order is important, distributed should be the last)\n","    if config.fp16:\n","        from apex import amp  # Apex is only required if we use fp16 training\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=config.fp16)\n","    if config.distributed:\n","        model = DistributedDataParallel(model, device_ids=[config.local_rank], output_device=config.local_rank)\n","\n","    logger.info(\"Prepare datasets\")\n","    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(config, tokenizer)\n","\n","    # Training function and trainer\n","    def update(engine, batch):\n","        model.train()\n","        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids, token_emotion_ids = tuple(input_tensor.to(config.device) for input_tensor in batch)\n","        #token_emotion_ids = None\n","        lm_loss, mc_loss = model(input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids, token_emotion_ids)\n","        loss = (lm_loss * config.lm_coef + mc_loss * config.mc_coef) / config.gradient_accumulation_steps\n","        if config.fp16:\n","            with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), config.max_norm)\n","        else:\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_norm)\n","        if engine.state.iteration % config.gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        return loss.item()\n","    trainer = Engine(update)\n","\n","    # Evaluation function and evaluator (evaluator output is the input of the metrics)\n","    def inference(engine, batch):\n","        model.eval()\n","        with torch.no_grad():\n","            batch = tuple(input_tensor.to(config.device) for input_tensor in batch)\n","            input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids, token_emotion_ids = batch\n","            #token_emotion_ids = None\n","            model_outputs = model(input_ids, mc_token_ids, token_type_ids=token_type_ids, token_emotion_ids=token_emotion_ids)\n","            lm_logits, mc_logits = model_outputs[0], model_outputs[1]  # So we can also use GPT2 outputs\n","            lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n","            lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n","            return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels)\n","    evaluator = Engine(inference)\n","\n","    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n","    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n","    if config.n_epochs < 1:\n","        trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))\n","    if config.eval_before_start:\n","        trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n","\n","    # Make sure distributed data samplers split the dataset nicely between the distributed processes\n","    if config.distributed:\n","        trainer.add_event_handler(Events.EPOCH_STARTED, lambda engine: train_sampler.set_epoch(engine.state.epoch))\n","        evaluator.add_event_handler(Events.EPOCH_STARTED, lambda engine: valid_sampler.set_epoch(engine.state.epoch))\n","\n","    # Linearly decrease the learning rate from lr to zero\n","    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, config.lr), (config.n_epochs * len(train_loader), 0.0)])\n","    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n","\n","    # Prepare metrics - note how we compute distributed metrics\n","    RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n","    metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-1), output_transform=lambda x: (x[0][0], x[1][0])),\n","               \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n","\n","    metrics.update({\"precision\": Precision(output_transform=lambda x: (x[0][1], x[1][1])),\n","                    \"recall\": Recall(output_transform=lambda x: (x[0][1], x[1][1]))})\n","\n","    metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], config),\n","                    \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], config)})\n","\n","    metrics.update({\"confusion_matrix\": ConfusionMatrix(num_classes=6, output_transform=lambda x: (x[0][1], x[1][1]))})\n","    metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n","    for name, metric in metrics.items():\n","        metric.attach(evaluator, name)\n","\n","    # On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n","    if config.local_rank in [-1, 0]:\n","        pbar = ProgressBar(persist=True)\n","        pbar.attach(trainer, metric_names=[\"loss\"])\n","        evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n","\n","        tb_logger = TensorboardLogger(log_dir=config.log_dir)\n","        tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n","        tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n","        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()), global_step_transform=global_step_from_engine(trainer)), event_name=Events.EPOCH_COMPLETED)\n","\n","        checkpoint_handler = ModelCheckpoint(tb_logger.writer.log_dir, 'checkpoint', save_interval=1, n_saved=3)\n","        trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # \"getattr\" take care of distributed encapsulation\n","\n","        torch.save(config, tb_logger.writer.log_dir + '/model_training_args.bin')\n","        getattr(model, 'module', model).config.to_json_file(os.path.join(tb_logger.writer.log_dir, CONFIG_NAME))\n","        tokenizer.save_vocabulary(tb_logger.writer.log_dir)\n","\n","    # Run the training\n","    trainer.run(train_loader, max_epochs=config.n_epochs)\n","\n","    # On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n","    if config.local_rank in [-1, 0] and config.n_epochs > 0:\n","        os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(tb_logger.writer.log_dir, WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n","        tb_logger.close()\n"],"metadata":{"id":"Mk_C15FwC4n3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"id":"YjCaVfs3EGrT","executionInfo":{"status":"error","timestamp":1669787618512,"user_tz":300,"elapsed":111,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"d0397338-e462-4d1a-841e-8093416b83c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Running process -1\n","ERROR:pytorch_pretrained_bert.tokenization_openai:Model name '/content/drive/MyDrive/EMChat/EmChat/checkpoint' was not found in model name list (openai-gpt). We assumed '/content/drive/MyDrive/EMChat/EmChat/checkpoint' was a path or url but couldn't find files /content/drive/MyDrive/EMChat/EmChat/checkpoint/vocab.json and /content/drive/MyDrive/EMChat/EmChat/checkpoint/merges.txt at this path or url.\n","ERROR:pytorch_pretrained_bert.modeling_openai:Model name '/content/drive/MyDrive/EMChat/EmChat/checkpoint' was not found in model name list (openai-gpt). We assumed '/content/drive/MyDrive/EMChat/EmChat/checkpoint' was a path or url but couldn't find files /content/drive/MyDrive/EMChat/EmChat/checkpoint/pytorch_model.bin and /content/drive/MyDrive/EMChat/EmChat/checkpoint/config.json at this path or url.\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-daa3273de397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-37aac99743ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIGPTDoubleHeadLMEmotionRecognitionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPECIAL_TOKENS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_num_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPECIAL_TOKENS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_special_tokens'"]}]}]}