{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7Lx30u6DT2WBrnLIdXeNK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQVYGgdq1JI7","executionInfo":{"status":"ok","timestamp":1670170191415,"user_tz":300,"elapsed":23629,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"c6febb97-257d-4eb8-d212-541d13a1fca1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = 'EMChat/EmChat'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"]},{"cell_type":"code","source":["!pip3 install pytorch-ignite\n","!pip3 install boto3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TgQXKTDy3ZxH","executionInfo":{"status":"ok","timestamp":1670170207560,"user_tz":300,"elapsed":16150,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"9c089c24-4d2c-49a4-ad25-218074390df9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-ignite\n","  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 32.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite) (21.3)\n","Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<2,>=1.3->pytorch-ignite) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->pytorch-ignite) (3.0.9)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.4.10\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting boto3\n","  Downloading boto3-1.26.22-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 23.1 MB/s \n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.22\n","  Downloading botocore-1.29.22-py3-none-any.whl (10.2 MB)\n","\u001b[K     |████████████████████████████████| 10.2 MB 64.1 MB/s \n","\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 9.7 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 73.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.22->boto3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.22->boto3) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.13 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.26.22 botocore-1.29.22 jmespath-1.0.1 s3transfer-0.6.0 urllib3-1.26.13\n"]}]},{"cell_type":"code","source":["# Copyright (c) 2019-present, HuggingFace Inc.\n","# All rights reserved. This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.\n","import os\n","import math\n","import logging\n","from pprint import pformat\n","from argparse import ArgumentParser\n","from collections import defaultdict\n","from itertools import chain\n","import torch.nn.functional as F\n","from config import InteractConfig\n","\n","import torch\n","from torch.nn.parallel import DistributedDataParallel\n","from torch.utils.data import DataLoader, TensorDataset\n","from ignite.engine import Engine, Events\n","from ignite.handlers import ModelCheckpoint\n","from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n","from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n","from config import Config\n","from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n","from ignite.contrib.handlers.tensorboard_logger import *\n","\n","from pytorch_pretrained_bert import (OpenAIAdam, OpenAIGPTMultiHeadModel, OpenAIGPTTokenizer,\n","                                     GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME,\n","                                     BertModel, BertTokenizer)\n","\n","from utils import get_dataset, get_dataset_for_daily_dialog\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ez01N1O3H0M","executionInfo":{"status":"ok","timestamp":1670170229145,"user_tz":300,"elapsed":21590,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"c3e00ecf-c3ba-4b2c-fa5c-5fcb7bfd5d07"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n","  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"]}]},{"cell_type":"code","source":[" #Final 40477\n","SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\",\n","\n","                  \"<no_emotion>\", \"<happiness>\", \"<surprise>\", \"<sadness>\", \"<disgust>\", \"<anger>\", \"<fear>\",\n","\n","                  \"<work>\", \"<finance>\", \"<relationship>\", \"<attitude_and_emotion>\", \"<culture_and_education>\",\n","                  \"<school_life>\", \"<tourism>\", \"<ordinary_life>\", \"<politics>\", \"<health>\",\n","\n","                  \"<directive>\", \"<inform>\", \"<commissive>\", \"<question>\",\n","                  \"<pad>\"]\n","MODEL_INPUTS = [\"input_ids\", \"ec_token_ids\", \"sc_token_ids\", \"lm_labels\", \"ec_labels\", \"sc_labels\",\n","                \"token_type_ids\", \"token_emotion_ids\", \"token_action_ids\"]\n","PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\", \"token_emotion_ids\", \"token_action_ids\"]\n","\n","# logger = logging.getLogger(__file__)"],"metadata":{"id":"CfungN9E3FXq","executionInfo":{"status":"ok","timestamp":1670170229145,"user_tz":300,"elapsed":8,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["logger = logging.getLogger()\n","fhandler = logging.FileHandler(filename='iteract_multihead.log', mode='a')\n","logger.addHandler(fhandler)\n","logging.warning('This is a warning message')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"keBvuaub32Uf","executionInfo":{"status":"ok","timestamp":1670170229145,"user_tz":300,"elapsed":7,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"e9dbf8f8-45d3-414c-976f-d28ba2ad69c4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:This is a warning message\n"]}]},{"cell_type":"code","source":["def average_distributed_scalar(scalar, config):\n","    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n","    if config.local_rank == -1:\n","        return scalar\n","    scalar_t = torch.tensor(scalar, dtype=torch.float, device=config.device) / torch.distributed.get_world_size()\n","    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n","    return scalar_t.item()\n","\n","\n","def pad_dataset(dataset, padding=0):\n","    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padd only batches but this is simpler. \"\"\"\n","    max_l = max(len(x) for x in dataset[\"input_ids\"])\n","    for name in PADDED_INPUTS:\n","        dataset[name] = [x + [padding if name != \"lm_labels\" else -1] * (max_l - len(x)) for x in dataset[name]]\n","    return dataset\n","\n","\n","def get_emotion_label(tokenizer, candidate_emotion):\n","    no_emotion_id, happiness_id, surprise_id, sadness_id, disgust_id, anger_id, fear_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[4:11])\n","\n","    if candidate_emotion == no_emotion_id:\n","        return 0\n","    elif candidate_emotion == happiness_id:\n","        return 1\n","    elif candidate_emotion == surprise_id:\n","        return 2\n","    elif candidate_emotion == sadness_id:\n","        return 3\n","    elif candidate_emotion == disgust_id:\n","        return 4\n","    elif candidate_emotion == anger_id:\n","        return 5\n","    elif candidate_emotion == fear_id:\n","        return 6\n","\n","def build_input_from_segments(topic, history, emotions, actions, reply, tokenizer, lm_labels=False, with_eos=True):\n","    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply \"\"\"\n","    bos, eos, speaker1, speaker2, no_emotion = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:5])\n","\n","    inform = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-4])\n","    emotions = [no_emotion] + emotions + [no_emotion]\n","    actions = [inform] + actions + [inform]\n","\n","    instance = {}\n","    sequence = [[bos] + [topic]] + history + [reply + ([eos] if with_eos else [])]\n","    sequence = [[speaker2 if (len(sequence) - i) % 2 else speaker1] + s for i, s in enumerate(sequence)]\n","\n","    instance[\"input_ids\"] = list(chain(*sequence))\n","    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in\n","                                  s]  # the last for is for repeating the speaker1 and speaker2 for all tokens\n","    instance[\"token_emotion_ids\"] = [emotions[i] for i, s in enumerate(sequence[:]) for _ in s] \n","\n","    instance[\"token_action_ids\"] = [actions[i] for i, s in enumerate(sequence[:]) for _ in s] \n","\n","    instance[\"ec_token_ids\"] = len(instance[\"input_ids\"]) - 1\n","    instance[\"sc_token_ids\"] = len(instance[\"input_ids\"]) - 2\n","    return instance, sequence\n","\n","\n","# def build_input_from_segments(topic, history, emotions, actions, reply, candidate_emotion,  canidate_act, tokenizer, lm_labels=False, with_eos=True):\n","#     \"\"\" Build a sequence of input from 3 segments: persona, history and last reply \"\"\"\n","#     bos, eos, speaker1, speaker2, no_emotion = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:5])\n","\n","#     inform = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-4])\n","#     emotions = [no_emotion] + emotions\n","#     actions = [inform] + actions\n","\n","#     instance = {}\n","#     sequence = [[bos] + [topic]] + history + [reply + ([eos] if with_eos else [])]\n","#     sequence = [[speaker2 if (len(sequence) - i) % 2 else speaker1] + s for i, s in enumerate(sequence)]\n","\n","#     instance[\"input_ids\"] = list(chain(*sequence))\n","#     instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in\n","#                                   s]  # the last for is for repeating the speaker1 and speaker2 for all tokens\n","#     instance[\"token_emotion_ids\"] = [emotions[i] for i, s in enumerate(sequence[:-1]) for _ in s] + [\n","#         candidate_emotion] * len(sequence[-1])\n","#     instance[\"token_action_ids\"] = [actions[i] for i, s in enumerate(sequence[:-1]) for _ in s] + [canidate_act] * len(\n","#         sequence[-1])\n","\n","#     instance[\"ec_token_ids\"] = len(instance[\"input_ids\"]) - 1\n","#     instance[\"sc_token_ids\"] = len(instance[\"input_ids\"]) - 2\n","#     instance[\"ec_labels\"] = -1\n","#     instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n","#     if lm_labels:\n","#         instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][\n","#                                                                                      1:]  # all -1 except for reply, reply is just the ids\n","#         instance[\"ec_labels\"] = get_emotion_label(tokenizer, candidate_emotion)\n","#     return instance, sequence\n","\n","def top_filtering(logits, top_k=0, top_p=0.0, threshold=-float('Inf'), filter_value=-float('Inf')):\n","    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n","        Args:\n","            logits: logits distribution shape (..., vocabulary size)\n","            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n","            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n","                whose total probability mass is greater than or equal to the threshold top_p.\n","                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n","                the threshold top_p.\n","            threshold: a minimal threshold to keep logits\n","    \"\"\"\n","    top_k = min(top_k, logits.size(-1))\n","    if top_k > 0:\n","        # Remove all tokens with a probability less than the last token in the top-k tokens\n","        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n","        logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","        # Compute cumulative probabilities of sorted tokens\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","        # Remove tokens with cumulative probability above the threshold\n","        sorted_indices_to_remove = cumulative_probabilities > top_p\n","        # Shift the indices to the right to keep also the first token above the threshold\n","        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","        sorted_indices_to_remove[..., 0] = 0\n","\n","        # Back to unsorted indices and set them to -infinity\n","        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","        logits[indices_to_remove] = filter_value\n","\n","    indices_to_remove = logits < threshold\n","    logits[indices_to_remove] = filter_value\n","\n","    return logits"],"metadata":{"id":"9SIpE-iv3CUp","executionInfo":{"status":"ok","timestamp":1670170229146,"user_tz":300,"elapsed":7,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["EMOTIONS = [\"NO_EMOTION\", \"HAPPINESS\", \"SURPRISE\", \"SADNESS\", \"DISGUST\", \"ANGRY\", \"FEAR\"]\n","EMOTIONS_TOKEN_ID = {\"NO_EMOTION\":40482, \"HAPPINESS\":40483, \"SURPRISE\":40484, \"SADNESS\":40485, \"DISGUST\":40486, \"ANGRY\":40487, \"FEAR\":40488}\n"],"metadata":{"id":"qyEbn2Sd28g_","executionInfo":{"status":"ok","timestamp":1670170229146,"user_tz":300,"elapsed":7,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def sample_sequence(history, tokenizer, model, args, SPECIAL_TOKENS, reply=None, emotion_history=None):\n","    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","\n","    if reply is None:\n","        reply = []\n","\n","    candidate_emotion = []\n","    candidate_act = []\n","\n","\n","    for i in range(args.max_length):\n","        topic = 40492\n","        emotions = emotion_history\n","        actions = [40499]*len(history)\n","        instance, sequence = build_input_from_segments(topic, history, emotions, actions, reply, tokenizer, SPECIAL_TOKENS,\n","                                                       with_eos=False)\n","\n","        input_ids = torch.tensor(instance[\"input_ids\"], device=args.device).unsqueeze(0).unsqueeze(0)\n","        ec_token_ids = torch.tensor(instance[\"ec_token_ids\"], device=args.device).unsqueeze(0).unsqueeze(0)\n","        sc_token_ids = torch.tensor(instance[\"sc_token_ids\"], device=args.device).unsqueeze(0).unsqueeze(0)\n","        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args.device).unsqueeze(0).unsqueeze(0)\n","        token_emotion_ids = torch.tensor(instance[\"token_emotion_ids\"], device=args.device).unsqueeze(0).unsqueeze(0)\n","        token_action_ids = torch.tensor(instance[\"token_action_ids\"], device=args.device).unsqueeze(0).unsqueeze(0)\n","\n","        logits, emotion_logits, sentence_logits  = model(input_ids, ec_token_ids, sc_token_ids, token_type_ids=token_type_ids,\n","                                  token_emotion_ids=token_emotion_ids,\n","                                  token_action_ids=token_action_ids)\n","\n","        emotion_indices = torch.argmax(emotion_logits, dim=2)\n","        \n","        logits = logits.squeeze(0)\n","        if \"gpt2\" == args.model:\n","            logits = logits[0]\n","        logits = logits[0, -1, :] / args.temperature\n","        logits = top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n","        if i < args.min_length and prev.item() in special_tokens_ids:\n","            while prev.item() in special_tokens_ids:\n","                prev = torch.multinomial(probs, num_samples=1)\n","\n","        if prev.item() in special_tokens_ids:\n","            break\n","        reply.append(prev.item())\n","        candidate_emotion.append(EMOTIONS[emotion_indices[0][0]])\n","\n","    return reply, candidate_emotion\n","\n"],"metadata":{"id":"GIksGBa92xDm","executionInfo":{"status":"ok","timestamp":1670170229146,"user_tz":300,"elapsed":6,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def run():\n","    # config_file = \"configs/interact_multihead_config.json\"\n","    config_file = \"/content/drive/MyDrive/EMChat/EmChat/configs/interact_multihead_config.json\"\n","    config = InteractConfig.from_json_file(config_file)\n","\n","    # logging.basicConfig(level=logging.INFO)\n","    # logger = logging.getLogger(__file__)\n","    # logger.info(pformat(config))\n","\n","    if config.model_checkpoint == \"\":\n","        config.model_checkpoint = download_pretrained_model()\n","\n","    torch.random.manual_seed(config.seed)\n","    torch.cuda.manual_seed(config.seed)\n","\n","    logger.info(\"Get pretrained model and tokenizer\")\n","    if config.model == \"bert\":\n","        tokenizer_class = BertTokenizer\n","        model_class = BertLMHeadModel\n","    elif config.model == \"gpt2\":\n","        tokenizer_class = GPT2Tokenizer\n","        model_class = GPT2LMHeadModel\n","    else:\n","        tokenizer_class = OpenAIGPTTokenizer\n","        model_class = OpenAIGPTMultiHeadModel\n","\n","    # SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n","\n","    tokenizer = tokenizer_class.from_pretrained(config.model_checkpoint)\n","    model = model_class.from_pretrained(config.model_checkpoint)\n","\n","    model.to(config.device)\n","    model.eval()\n","    out_ids = None\n","    candidate_emotion = None\n","    emotion_history = []\n","    history = [] \n","    while True:\n","        raw_text = input(\">>> \")\n","        while not raw_text:\n","            print('Prompt should not be empty!')\n","            raw_text = input(\">>> \")\n","        if out_ids != None:\n","            history.append(out_ids)\n","            emotion_history.append(EMOTIONS_TOKEN_ID[candidate_emotion[len(candidate_emotion)-1]])\n","        history.append(tokenizer.encode(raw_text))\n","        emotion_history.append(EMOTIONS_TOKEN_ID[\"HAPPINESS\"])\n","        with torch.no_grad():\n","            out_ids, candidate_emotion = sample_sequence(history, tokenizer, model, config, SPECIAL_TOKENS, None, emotion_history)\n","        history = history[-(2 * config.max_history + 1):]\n","        emotion_history = emotion_history[-(2 * config.max_history + 1):]\n","        out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n","        print(f\"{candidate_emotion[len(candidate_emotion)-1]} : {out_text}\")\n","\n","\n"],"metadata":{"id":"Wp2s_4hp2xPu","executionInfo":{"status":"ok","timestamp":1670170229146,"user_tz":300,"elapsed":6,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ifGHSWp72xaK","executionInfo":{"status":"error","timestamp":1670170426275,"user_tz":300,"elapsed":197135,"user":{"displayName":"Jai N Sharma","userId":"10573252836174815531"}},"outputId":"855e2a64-bf63-4ddf-c1bb-7bd36321ec67"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pytorch_pretrained_bert.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"]},{"output_type":"stream","name":"stdout","text":[">>> tap\n","NO_EMOTION : no.\n",">>> smile\n","FEAR : yes, sir.\n",">>> how are you\n","NO_EMOTION : i'm doing?\n",">>> yes\n","NO_EMOTION : i'm doing very well.\n",">>> nice to know\n","NO_EMOTION : thank you.\n",">>> how is the weather\n","NO_EMOTION : it's very nice.\n",">>> why\n","NO_EMOTION : it's very hot.\n",">>> why\n","HAPPINESS : the temperature is always below zero.\n",">>> why\n","HAPPINESS : because it's very hot in the summer.\n",">>> why\n","NO_EMOTION : because it's very hot in the winter.\n",">>> why\n","NO_EMOTION : because it's very hot in the winter.\n",">>> car\n","NO_EMOTION : i see.\n",">>> colour\n","NO_EMOTION : how much is it?\n",">>> 500\n","NO_EMOTION : it's a lot of money.\n",">>> it is cheap\n","NO_EMOTION : how much is it?\n",">>> 1\n","NO_EMOTION : 500 dollars.\n",">>> 2\n","NO_EMOTION : 500 dollars.\n",">>> 3'\n","NO_EMOTION : here is the money.\n",">>> thanks\n","NO_EMOTION : you're welcome.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-4a5b8788c965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-e5c5e5f57a3f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prompt should not be empty!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}]}